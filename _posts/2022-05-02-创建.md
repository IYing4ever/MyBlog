---
layout: mypost
title: 属性多重异质网络的表示学习
categories: [Jekyll]
extMath: true
---

# 属性多重异质网络的表示学习


![cbai](https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-05%2000.14.28.png)

------

[TOC]

------





### 摘要

网络嵌入(或图嵌入)在现实世界中有着广泛的应用。然而，现有的方法主要集中在单一类型节点/边的网络上，不能很好地扩展以处理大型网络。许多现实世界的网络由数十亿个节点和多种类型的边组成，每个节点都与不同的属性相关联。在本文中，我们使嵌入学习的多重异质属性网络的问题正式化，并提出一个统一的框架来解决这个问题。这个框架既支持[转导学习（直推学习），也支持归纳学习](https://t.cj.sina.com.cn/articles/view/6903666503/19b7d974700100o90f?sudaref=cn.bing.com&display=0&retcode=0)。并对该框架进行了理论分析，说明了其与先前工作（其他已提出模型）的联系，证明了其更好的表现。我们在Amazon、YouTube、Twitter和Alibaba1四种不同类型的有挑战性的数据集上对提议的框架进行了系统的评估。实验结果表明，通过从所提出的框架中学习到的嵌入，我们可以实现==统计==上显著的改进(例如，[F1分数](https://baike.baidu.com/item/F1分数/13864979)提高5.99-28.23%;p << 0.01, t检验（t检验的概率远小于0.01，不能拒绝原假设）)超过了以前最先进的链接预测方法。该框架已成功应用于全球领先的电子商务公司阿里巴巴集团的推荐系统中。线下产品推荐的A/B测试结果进一步验证了该框架在实际应用中的有效性和高效性。

### 关键字

网络嵌入；多重网络；异质网络

### 1. 引言

网络嵌入[4]，或网络表示学习，是一种很有前途的将网络中的节点投影到低维连续空间中，同时保持网络结构和固有属性的方法。近年来，下游网络学习任务如节点分类[1]、链路预测[39]、社区检测[8]等取得了显著进展，引起了人们的极大关注。DeepWalk[27]、LINE[35]和node2vec[10]是将深度学习技术引入网络分析来学习节点嵌入的先驱作品。NetMF[29]对不同的网络嵌入算法进行了等价性的理论分析，随后NetSMF[28]通过稀疏化给出了可扩展的解决方案。然而，它们被设计成只处理具有单一类型节点和边的同质网络。最近，PTE[34]、metapath2vec[7]和HERec[31]被提出用于异质网络。然而，现实世界的网络结构应用程序(如电子商务)要复杂得多，它不仅包含多种类型的节点和边，而且还包含一组丰富的属性。由于它的重要意义和挑战性的要求，在文献中已经有大量的尝试来研究复杂网络的嵌入学习。根据网络拓扑结构(同质或异质)和属性特征(有属性或没有属性)，我们将六种不同类型的网络进行分类，并分别总结了它们的相对综合发展，见表1。这六种类型包括同质网络(HON)、属性同质网络(AHON)、异质网络(HEN)、属性异质网络(AHEN)、多重异质网络(MHEN)和属性多重异质网络(AMHEN)。可以看出，其中对AMHEN的研究最少。
 <img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-23%2015.27.36.png" alt="截屏2021-02-23 15.27.36" style="zoom:50%;" />

在这篇论文中，我们重点研究了AMHEN的嵌入学习，其中，不同类型的节点可能与多种不同类型的边相连接，每个节点都与一组不同的属性相关联。这在许多在线申请中很常见。例如，在我们正在处理的四个数据集中，20.3% (Twitter)、21.6% (YouTube)、15.1%(亚马逊)和16.3%(阿里巴巴)的链接节点对分别有一种以上的边。举例来说，在电子商务系统中，用户可能与商品有几种类型的交互，例如单击、切换、加入购物车、加入收藏。图1演示了这样一个示例。显然，“用户”和“物品”具有本质上不同的属性，不应该被平等对待。此外，不同的用户-物品交互意味着不同层次的兴趣，应该区别对待。否则，系统就不能准确地捕捉用户的行为模式和偏好，无法满足实际使用。

<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-01%2023.07.46.png" alt="截屏2021-02-01 23.07.46" style="zoom:50%;" />

*图1:左边说明了一个多重异质属性网络的例子。图左边的用户与属性相关联，包括性别、年龄和位置。同样，图左部分的商品包括诸如价格和品牌等属性。用户和商品之间的边缘类型来自四种交互，包括单击、添加到收藏、添加到购物车和转换。中间的三个子图代表三种不同的图表设置方式，从下到上依次是HON、MHEN、AMHEN。右边显示了模型DeepWalk在Alibaba数据集上的性能提升。可以看出，与DeepWalk相比，GATNE-I的性能提升为+28.23%*

不仅因为AMHEN的异质性和多重性，在实践中，处理AMHEN还面临着几个独特的挑战:

* Multiplex Edges（多重边）. 每个节点对可能有多种不同类型的关系。能够从不同的关系中借鉴优势并学习统一嵌入是很重要的。
* Partial Observations（局部观察）. 真实的网络数据通常是局部观测到的。例如，长尾客户可能只与某些产品进行很少的交互。现有的网络嵌入方法大多集中在直推上，不能处理长尾问题或冷启动问题。

* 可扩展性。真实的网络通常有数十亿个节点和数百亿或数千亿条边[40]。开发能够很好地扩展到大型网络的学习算法是非常重要的。

为了解决上述挑战，我们提出了一种新颖的方法来捕获丰富的属性信息，并利用来自不同节点类型的多重拓扑结构，即通用属性多重异质网络嵌入，或简称为GATNE。GATNE的主要特点如下:

* 我们正式定义了属性多重异质网络嵌入问题，它是现实网络的一个更普遍的表示。

* GATNE支持属性多重异质网络的直推嵌入学习和归纳嵌入学习。通过理论分析，证明了我们的直推模型比现有的模型(如MNE[43])更为普遍。

* 针对GATNE开发了高效、可扩展的学习算法。我们的学习算法能够有效地处理数亿个节点和数十亿条边。

我们进行了大量的实验来评估提议的模型在四种不同类型的数据集:亚马逊，YouTube, Twitter和阿里巴巴。实验结果表明，所提出的框架在统计上有显著的提升(在阿里巴巴数据集上F1得分提升5.99- 28.23%;p≪0.01,t−test)通过最先进的方法。我们将该模型部署在阿里巴巴的分布式系统上，并将该方法应用到阿里巴巴的推荐引擎中。离线A/B测试进一步验证了我们所提出模型的有效性和效率。

### 2. 相关工作

在这一节中，我们回顾了网络嵌入、异质网络嵌入、多路异质网络嵌入和属性网络嵌入的相关研究进展。

**网络嵌入**。网络嵌入的工作主要包括两大类:图嵌入(GE)和图神经网络(GNN)。GE的代表作有DeepWalk[27]，该算法通过随机漫步的方式在图形上生成语料库，然后在语料库上训练skip-gram跳跃模型。LINE[35]学习大规模网络中的节点表示，同时保持一阶和二阶近似性。node2vec[10]设计了一个有偏随机漫步程序来有效地探索不同的邻域。NetMF[29]是一个统一的矩阵分解框架，用于理论理解和改进DeepWalk和LINE。对于GNN中的流行作品，GCN[19]使用卷积运算将邻居特征表示合并到节点特征表示中。GraphSAGE[11]提供了一种将结构信息与节点特征结合起来的归纳方法。它学习函数表示而不是每个节点的直接嵌入，这有助于它在训练过程中对未观察到的节点进行归纳工作。

**异质网络嵌入** 异质网络检查具有各种类型的节点和/或边缘的场景。这种网络是出了名的难以挖掘，因为混杂的内容和结构令人迷惑。创建此类数据的多维嵌入为使用多种现成的多维数据挖掘技术打开了大门。尽管这个问题很重要，但在嵌入一个可扩展的动态异质数据网络方面所做的努力有限。HNE[3]综合考虑了网络中的内容和拓扑结构，将异质网络中的不同对象表示为统一的向量表示。PTE[34]将标签信息和不同级别的词共现信息构建成大规模的异质文本网络，并将其嵌入到低维空间中。metapath2vec[7]形式化了基于随机漫步的元路径来构造节点的异质邻域，然后利用异质跳gram模型来实现表单节点嵌入。这里[31]使用基于元路径的随机-dom漫步策略来生成有意义的节点序列，以学习网络嵌入，首先通过一组融合函数进行转换，然后集成到扩展矩阵分解(MF)模型中。

**多重异质网络嵌入** 现有的方法通常研究具有单一类型的节点之间的邻近性的网络，这种网络只捕获一个网络的单一视图。然而，在现实中，节点之间通常存在多种类型的临近性，从而产生具有多个视图的网络，或多路网络嵌入。PMNE[22]提出了将多路网络投影到连续向量空间中的三种方法。MVE[30]使用注意机制将多个视图的网络嵌入到一个协作嵌入中。MNE[43]对每个节点使用每种边缘类型的一个通用嵌入和多个附加嵌入，通过统一的网络嵌入模型共同学习。Mvn2vec[32]探讨了通过同时建模保存和协同表示不同视图中边缘的语义意义来实现更好嵌入效果的可行性。

**属性网络嵌入** 属性网络嵌入的目的是寻找网络中节点的低维向量表示，在这种表示中保留原有的网络拓扑结构和节点属性的邻近性。TADW[41]在矩阵分解的框架下，将顶点的文本特征引入到网络表示学习中。LANE[16]平滑地将标签信息嵌入到属性网络中，同时保持它们的相关关系。AANE[15]使联合学习过程能够以分布式的方式完成，以加速归属网络嵌入。SNE[20]提出了一个通用的框架，通过捕获结构接近性和属性接近性来嵌入社交网络。DANE[9]能够捕获高度非线性，并保留拓扑结构和节点属性的各种接近性。ANRL[44]采用邻居增强自动编码器对节点属性信息进行建模，并采用基于属性编码器的属性感知的skip-gram模型对网络结构进行捕获。

### 3. 问题定义

表示一个网络$\ G=(\mathcal{V}, \mathcal{E})\ $，其中$\mathcal{V}$是$\mathcal{n}$个节点的集合，$\mathcal{E}$是节点之间的边的集合。每条边$\ e_{ij} = (v_i,v_j)\ $关联一个权值$w_{ij}\ge0$，表示vi和vj之间关系的强度。在实践中，网络可以是有向的，也可以是无向的。如果G有定向，我们有$e_{ij}\ne eji$和$wij \ne wji$;如果G是无向的，我们有$e_{ij}= e_{ji}$和$w_{ij} = w_{ji}$。表2总结了表示法。

<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-01%2023.43.46.png" alt="截屏2021-02-01 23.43.46">

|                         符号 | 描述                              |
| ---------------------------: | :-------------------------------- |
|                          $G$ | 输入网络                          |
| $\mathcal{V}$, $\mathcal{E}$ | G的节点/边的集合                  |
|             $\mathcal{O, R}$ | G的节点/边的类型的集合            |
|                $\mathcal{A}$ | G的属性集合                       |
|                          $n$ | 节点数量                          |
|                          $m$ | 边类型的数量                      |
|                          $r$ | 一个边类型                        |
|                          $d$ | 基本/全部嵌入的维数               |
|                          $s$ | 边嵌入的维数                      |
|                            𝒗 | 图中的一个节点                    |
|                $\mathcal{N}$ | 一个节点在一个边类型上的邻域集    |
|       $\mathcal{b, u, c, v}$ | 一个节点的基本/边/上下文/整体嵌入 |
|                       $h, g$ | 变换函数在我们的归纳方法中        |
|                 $\mathbf{x}$ | 节点的属性                        |

定义1(异质网络)。异质网络[3,33]是一个网络 $G = (\mathcal{V, E})$与一个节点类型映射函数$\phi: \mathcal{V} \rightarrow \mathcal{O}$和一个边类型映射函数$\ \psi : \mathcal{E} \to \mathcal{R}\ $相关联，其中$\ \mathcal{O}\ $和$\ \mathcal{R}\ $分别表示所有节点类型的集合和所有边类型的集合。每个节点$\ v \in \mathcal{V}\ $属于特定的节点类型。同样，每条边$\ e \in \mathcal{E}\ $分为特定的边类型。如果$ |O| + |R|  \gt 2$，叫做网络异质;否则同质。

注意，在异质网络中，由于节点$v_i$和$v_j$之间可能存在多种类型的边e，因此不能再表示为$e_{ij}$。在这种情况下，一条边记为$e^{(r)} _{ij}$，其中$r$对应某一种边类型。

定义2(属性网络)。一个属性网络[3,16]是赋予属性A表示一个网络G，即$G = (V, E, A)$，每个节点$v_i ∈V$都与某些类型的特征向量相关联。$A = $ {$x_i |v_i∈V$}是所有节点的节点特征集，其中$x_i$是节点$v_i$的关联节点特征。

定义3(属性多重异质网络)。基于属性的多重异质网络是一个网络结构，即网络$G = (V, E, A)$，$ E = U _{r∈R} E_r$，其中$E_r$由所有边组成
边型$r∈R$，且$| R | > 1$。我们将每种边类型或***视图***view r∈R的网络分离为$G_r = (V, E_r, A)$。

图1展示了多重异质属性网络（AMHEN）的一个例子，它包含2种节点类型和4种边缘类型。这两种节点类型是user（用户）和item（商品），它们具有不同的属性。根据上述定义，我们可以正式地定义网络上表示学习的问题。

问题1 (AMHEN嵌入)。给定一个多重异质属性网络 G = (V, E, A), AMHEN嵌入的问题是对每个节点v在每个r型边上给出一个统一的低维空间表示。我们的目标是找到一个函数$f_r: V→ℝ^d$ 为每条r类型边, 当$d≪ | V |$时。

### 4 方法

在本节中，我们首先解释在直推模型[19]中提出的GATNE框架。生成的模型称为GATNE-T。我们还从理论上讨论了GATNE-T与MNE等最新趋势模型的联系。为了解决归纳问题，我们进一步将模型扩展到归纳环境[42]，并提出了一个新的归纳模型GATNE-I。对于这两种模型，我们给出了有效的优化算法。
<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-09%2015.48.50.png" alt="截屏2021-02-09 15.48.50" style="zoom:50%;">

***图2:GATNE-T和GATNE-I模型的说明。GATNE-T只使用网络结构信息，GATNE-I同时考虑结构信息和节点属性。异质skip-gram的输出层为输入节点v的每个临域节点类型指定了一组多项分布。在本例中，节点集合$V = V_1 \bigcup V_2 \bigcup V_3$和$K_1 、K_2 、K_3$分别在每个节点类型表示了节点v的邻域的大小。（三种类型的节点，分别表示为$V_1、V_2、V_3$）***

#### 4.1	直推模型: GATNE-T

我们首先在直推环境中为属性多重异质网络嵌入学习，并提出我们的模型GATNE-T。具体来说，在GATNE-T中，我们将某一节点$v_i$在每个边缘类型r上的整体嵌入分为base embedding和edge embedding两部分，如图2所示。不同edge type之间共享节点$v_i$的base embedding。将节点$v_i$在边缘类型r上的第k级edge embedding $u^{(k)}_{i,r}∈R^s$，(1≤ k ≤ K)，由相邻节点的edge embedding聚合为：

$$
u^{(k)}_{i,r} = aggregator(\{u^{(k−1)}_{j,r} , ∀v_j ∈ N_{i,r} \})
$$
其中，$N_{i,r}$是节点$v_i$在边缘类型r上的邻居集合。每个节点$v_i$和每个边缘类型r的初始edge embedding $u^{(0)}_{i,r}$在我们的直推模型中随机初始化。下列[Graph-SAGE](https://www.jianshu.com/p/3aa5a4a574e2) [11]，aggregator（聚合器）的功能可以作为一个均值聚合器：

$$
u^{(k)}_{i , r} = \sigma (\hat W^{(k)} \cdot mean(\{u^{(k-1)}_{j , r}, \forall v_j \in N_{i , r}\}))
$$


或者其他的pooling aggregator，比如max-pooling聚合器：

$$
u^{(k)}_{i , r} = max(\{\sigma (\hat W^{(k)}_{pool}u^{(k-1)}_{j , r} + \hat b^{(k)}_{pool}), \forall v_j \in N_{i , r}\})
$$


其中σ是激活函数。我们将K层edge embedding $u^{(K)}_{i,r}$表示为边缘嵌入$u_{i,r}$，将节点$v_i$的所有edge embedding连接为$U_i$，大小为s × m，其中s为edge embedding的维数（m为edge type的数量；注：**==k层也就是说聚合了k次==**）：

$$
$U_i = (u_{i,1} , u_{i,2}, \ldots , u_{i,m})$
$$
我们利用自注意机制(self-attention mechanism)[21]在edge type r上来计算$U_i$中向量线性组合的系数$a_{i,r} \in  R^m$：

$$
a_{i,r} = softmax(w^T_r tanh(W_rU_i))^T
$$


其中$w_r$和$ W_r$分别是大小为$d_a$和$d_a \ast s$的r类型边的可训练参数（矩阵/向量），上标T表示向量或矩阵的转置。因此，对于边缘类型r，节点$v_i$的整体嵌入量为:

$$
v_{i,r} = b_i + \alpha_rM^T_r U_i a_{i,r}
$$


其中$b_i$是节点$v_i$的base embedding，$\alpha_r$是表示边缘嵌入对整体嵌入（base embedding 和 edge embedding）重要性的超参数，$M_r \in R^{s*d}$是可训练的变换矩阵(edge type)。

**与之前工作的联系。**我们选择MHEN（多重异质网络）最近的代表作MNE[43]（多重网络嵌入）作为MHEN的基础模型，来讨论我们所提出的模型与之前工作之间的联系。在GATNE-T中，我们利用==注意机制==来捕捉不同边缘类型之间的影响因素。我们从理论上证明了我们的直推模型是MNE的一种更普遍的形式，并提高了MNE的表达能力。对于MNE，节点$v_i$在边缘类型$r \in R$上的整体嵌入$ \hat v_{i,r}$为
$$
\hat v_{i,r} = b_i + \alpha_r X^T_r o_{i,r}
$$


其中$X_r$为特定边(edge-specific)变换矩阵。对于GATNE-T，节点$v_i$在边缘类型r上的整体嵌入为:

$$
v_{i,r} = b_i + \alpha_rM^T_rU_ia_{i,r} = b_i + \alpha_rM^T_r \sum^m_{p=1} \lambda_pu_{i,p}
$$


其中$λ_p$表示$a_{i,r}$的第p个重素，计算方式为:

$$
\lambda_p = \cfrac {exp(w^T_r tanh(W_r u_{i,p}))}{\sum_t exp(w^T_r tanh(W_ru_{i,t}))}
$$


==定理4.1==。对于任意$r \in R$，存在参数$w_r$和$W_r$，使得对于任意$o_{i,1}, \cdots ， o_{i,m}$，以及对应的矩阵$X_r$，每个$o_{i,j}$的维数为s, $X_r$的大小为$s * d$，存在$u_{i,1}, \cdots ， u_{i,m}$，以及相应的矩阵$M_r$，每个$u_i$的维数为$s + m$，$M_r$大小为$(s + m) * d$  ，满足$v_{i, r} \approx \hat v_{i,r}$。

证明。对于任意t，让$u_{i,t} = (o^T_{i,t}, u^{'T }_{i,t}) ^T$由两个向量连接，其中第一个s维是$o_{i t}$,$u^{'}_{i,t}$是m维向量。设$u^{'}_{i,t,k}$为$u^{'}_{i,t}$的第k维，则取$u^{'}_{i,t,k} = δ_{tk}$，其中$\delta$为克罗内克函数，且

$$
\delta_{i j} = 1,i = j; \delta_{i j} = 0, i \ne j.
$$


设$W_r$除第1行和$s+r$列中M足够大的重素外，其余均为0;因此$tanh(W_ru_{i,p})$成为一个向量，其第一维近似为$δ_{rp}$，其他维数为0。取$w_r$为向量，第一维为M, $exp(w^T_r tanh (W_ru_{i,p})) \approx  exp(Mδ_{rp})$，则$λ_p \approx δ_{rp}$。最后，我们取$M_r$在第一个$s * d$维为$X_r$，在下一个$m * d$维为0，我们可以得到$v_{i, r} \approx \hat v_{i,r}$。

因此，MNE的参数空间几乎包含在我们模型的参数空间中，如果可以直接训练边缘嵌入，我们可以得出GATNE-T是MNE的一种推广。然而，在我们的模型中，边缘嵌入u是由单个或多个聚合层生成的。我们将对聚合案例进行更多的讨论。

**聚合的影响。**在GATNE-T模型中，edge embedding $u^{(k)}$的计算是通过聚合其邻居的edge embeding $u^{(k-1)}$来实现的:

$$
u^{(k)}_{i,r} = \sigma(\hat W^{(k)} \cdot mean(\{u^{(k-1)}_{j,r},v_j \in N_{i,r}\}))
$$


均值聚合器基本上就是一个矩阵乘法，

$$
mean_k(v_i) = mean(\{u^{(k-1)}_{j,r},v_j \in N_{i,r}\}) = (U^{(k-1)}_rn_r)_i
$$


其中$N_r$为edge type r上的邻域矩阵，$U^{(k)} = (u^{(k)}_{1,r},\cdots,u^{(k)}_{n,r})$图中所有节点在edge type r上的第k级edge embedding,$(U^{(k-1)}N_r)_i$为$U^{(k-1)}_rN_r$第i列，$N_r$可以是归一化邻接矩阵。式(11)的均值算子可以加权，邻接矩阵$N_{i,r}$可以采样。取$\hat W^{(k)} = I$，其中$I$是单位矩阵，则$u^{(k)}_{i, r} = \sigma(mean_k(v_i))$。如果$N_r$是满秩的，那么对于任意$O_r = (o_{1,r}, \cdots,o_{n,r},$存在$U^{(k-1})$使$U^{(k1)}_r N_r = O_r$.

如果激活函数σ是自同质，即$σ: R \rightarrow R$和$N_r$是满秩的，我们可以用定理4.1中描述的构造方法构造$u_{i,r}$，然后用上述方法构造每一级（层）edge embedding $U^{(k-1)}_{i,r},\cdots,u^{(0)}_{i,r}$。因此，当所有的邻域矩阵$N_r$和激活函数σ在所有级（层）的聚合中都是可逆的时，我们的模型仍然是可以推广MNE模型的更一般的形式。

#### 4.2	归纳模型:GATNE-I

GATNE-T的限制是它不能处理未观察到的数据（==注：也就是加入新的节点==）。然而，在许多实际应用中，网络数据通常是部分观察到的[42]。然后，我们将我们的模型扩展到归纳上下文，并提出了一个名为GATNE-I的新模型。图2也说明了该模型。具体来说，我们将base embedding $b_i$定义为$v_i$的属性$x_i$的参数化函数$b_i = h_z(x_i)$，其中$h_z$是一个变换函数，$z = \phi(i)$ 是节点$v_i$对应的节点类型。注意，不同类型的节点可能具有不同的属性$x_i$维度，变换函数$h_z$可以有不同的形式，例如一个多层感知器[26]。同样,最初的边缘嵌入$u^{(0)}_{i,r}$对于节点i在边缘类型r上还应该参数化的功能属性$x_i$作为 $u^{(0)}_{i,r} = g_{z,r}(x_i)$, $g_{z,r}$也是一个转换函数,它将节点$v_i$的特征转换为在边缘类型r上的嵌入，z是$v_i$对应的节点类型。具体来说，对于归纳模型，我们还在类型r上的节点$v_i$的整体嵌入中添加了一个额外的属性项:

$$
v_{i,r} = h_z(x_i) + \alpha_rM^T_rU_ia_i + \beta_rD^T_zx_i
$$


其中$β_r$是系数，$D_z$是$v_i$对应节点类型z上的特征变换矩阵。我们的直推模型和归纳模型的区别主要在于base embedding $b_i$和初始edge embedding $u^{(0)}_{i,r}$是如何生成的。在我们的直推模型中，根据网络结构对每个节点直接训练base embedding $b_i$和初始边edge embedding $u^{(0)}_{i,r}$，直推模型不能处理训练过程中未看到的节点。至于我们的归纳模型,而不是对每个节点直接训练$b_i$和$u^{(0)}_{i,r}$,我们训练转换函数$h_z$和$g_{z,r}$，转换原始特性$x_i$到$b_i$和$u^{(0}_{i,r})$，那些对节点的工作并没有出现在训练期间,只要他们有相应的原始特性。

#### 4.3	模型优化

我们讨论了如何学习所提出的直推和归纳模型。在[node2vec10, deepwalk27, line35]之后，我们使用随机游走来生成节点序列，然后对节点序列进行[skip-gram](https://zhuanlan.zhihu.com/p/27234078)[24,25]来学习嵌入。由于输入网络的每个子图都是异质的，所以我们使用基于元路径的随机游走[7]。具体来说，给定r子图，即$G_r = (V,\Epsilon_r,A)$和(meta-path)元路径方案$T:V_1 \to V_2 \to \cdots V_t \cdots \to V_l$，其中$l$为元路径方案长度，在第$t$步的转移概率定义如下:
$$
p(v_j｜v_i, T) = 
\begin{cases}
\cfrac 1{|N_{i,r}\cap V_{t+1} |}&\text{($v_i,v_j) \in E_r,v_j \in V_{t+1}$,} \\
0 &\text{$(v_i,v_j) \in E_r,v_j \notin  V_{t+1}$}\\
0 &\text{$(v_i,v_j)\notin E_r,$}
\end{cases}
$$

$v_i \in V_t$并且$N_{i,r}$表示节点$v_i$在edge type r上的邻域。行走器的流程以预定义的元路径$T$为条件。基于元路径的随机游走策略确保不同类型的节点之间的语义关系可以恰当融入[skip-gram](https://developer.aliyun.com/article/702385)模型[7]。假设在类型为r的边上，长度为l的随机游走遵循一条路径$P = (v_{p_1},...,v_{p_l})$，路径为$(v_{p_{t-1}},v_{p_t})\in E_r$，表示$v_{p_t}$的上下文为$C = \{v_{p_k} |v_{p_k} \in P， |k-t| ≤ c, t ≠ k\}$，其中c为窗口大小的半径(c越大，效果越好，相应的时间越长)。

因此，给定一个节点$v_i$及其路径上下文C，我们的目标是最小化下面的负对数似然函数：

$$
-log P_\theta(\text{ {$v_j|v_j \in C $}|$v_i$}) = \sum_{v_j\in C} - logP_\theta(v_j|v_i)
$$


θ表示所有参数。跟metapath2vec[7]一样，我们使用异质softmax函数，该函数根据节点$v_j$的节点类型进行了规范化。具体来说，$v_j$在给定$v_i$的情况下的概率定义为:

$$
P_\theta(v_j|v_i) = \frac{exp(c^T_j\cdot v_{i,r})}{\sum_{k \in V_t}exp(c^T_k\cdot v_{i,r})}
$$


其中$v_j \in V_t$,$ c_k$是节点$v_k$的上下文嵌入,$v_i
$是节点$v_i$对于edge type r的overall embedding。

最后，对每个节点对$(v_i,v_j)$使用异构负采样逼近目标函数$-log P_θ(v_j |v_i)$:

$$
E = -log\ \sigma(c^T_j\cdot v_{i,r}) - \sum^L_{l=1}E{_{v_k \sim P_t(v)}}[log \sigma(-c^T_k \cdot v_{i,r})]
$$


其中$σ(x) = 1/(1+e^{-x})$是sigmoid函数，$L$是负样本数量，相当于正训练样本，$v_k$是从节点$v_j$对应节点集$V_t$定义的噪声分布$P_t(v)$中随机抽取的。

我们在算法1中总结我们的算法。基于随机游走算法的时间复杂度$O(nmdL)$其中n是节点的数量,m是边缘类型的数量,d是整体嵌入大小,L是负样本每个训练样本的数量(L≥1)。算法的空间复杂度是$O(n (d + m×s))$和s为edge embedding的大小（维度）。

<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-10%2022.17.01.png" alt="截屏2021-02-10 22.17.01">

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h1f26zhil2j20yr0u0whn.jpg)





### 5	实验

在这一节中，我们首先介绍四个评估数据集的细节和竞争算法（对手）。我们将重点放在链路预测任务上，以评估我们所提出的方法与其他最先进的方法的性能。然后讨论了参数的灵敏度、收敛性和可扩展性。最后，我们报告了我们的方法在阿里巴巴推荐系统上的线下A/B测试结果。

#### 5.1	数据集

<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-10%2020.52.17.png" alt="截屏2021-02-10 20.52.17">

我们在三个公共数据集和阿里巴巴数据集上进行链接预测任务。[亚马逊产品数据集$^2$](http://jmcauley.ucsd.edu/data/amazon/)[13,23]包括产品重数据和产品之间的链接；[YouTube数据集](http://socialcomputing.asu.edu/datasets/YouTube)$^3$[36,38]包含各种类型的交互；[Twitter数据](https://snap.stanford.edu/data/higgs-twitter.html)集$^4$[6]也包含各种类型的链接。阿里巴巴数据集有两种节点类型，用户和商品(或产品)，用户和商品之间的交互包括四种类型。由于一些基线不能扩展到整个图，我们评估在采样数据集上的性能。这四个采样数据集的统计结果汇总在表3中。注意，表中的n-types和e-types类型分别表示节点类型和边缘类型。

**Amazon** 在我们的实验中，我们只使用电子产品类别的产品重数据，包括产品属性和产品之间的共同购买链接。产品属性包括价格、销售等级、品牌和类别。

**YouTube** YouTube数据集是一个多路双向网络数据集，包含15088个YouTube用户之间的五种交互类型。边缘的类型包括联系人、共享好友、共享订阅、共享订阅者和用户之间共享喜爱的视频。

**Twitter** 推特数据集是关于2012年7月1日至7日与希格斯玻色子发现相关的推文。它由超过45万推特用户之间的四个方向关系组成。这些关系包括Twitter用户之间的转发、回复、提及和友谊/追随者关系。

**Alibaba** 阿里巴巴数据集由四种类型的交互组成，包括点击、添加到偏好、添加到购物车，以及用户和商品两种类型节点之间的转换。采样后的阿里巴巴数据集记为Alibaba-S。在阿里巴巴的分布式云平台上对整个数据集进行了评估;将整个数据集记为阿里巴巴。

#### 5.2	Competitors（竞争对手）

我们把我们的竞争对手分为以下四类。所有方法的整体嵌入大小都设置为200。附录中列出了不同方法的具体超参数设置。

**网络嵌入方法** 比较的方法包括DeepWalk[27]、LINE[35]和node2vec[10]。由于这些方法只能处理HON（同质/同质网络），我们将具有不同边缘类型的单独的图输入给它们，并对每个单独的图获得不同的节点嵌入。

**异质网络嵌入方法 ** 我们将重点放在代表性工作metapath2vec[7]上，它被设计用来处理节点异质性。当网络中只有一种节点类型时，metapath2vec会退化为DeepWalk。对于Alibaba数据集，元路径方案设置为U-I-U和I-U-I, U和I分别表示用户和物品。

**多重异质网络嵌入方法** 比较方法包括PMNE[22]、MVE[30]、MNE[43]。PMNE的三种方法分别表示为PMNE(n)、PMNE(r)和PMNE(c)。MVE使用协作上下文嵌入，并对视图特定的嵌入应用注意机制。MNE对每种边缘类型使用一个通用嵌入和几个附加嵌入，通过统一的网络嵌入模型共同学习。

**属性网络嵌入方法** 比较方法为ANRL[44]。ANRL使用相邻增强自动编码器来建模节点属性信息，并使用基于属性编码器的属性感知、skip-gram模型来捕获网络结构。

**我们的模型** 我们建议的方法包括GATNE-T和GATNE-I。GATNE-T考虑网络结构，采用base embedding和edge embedding来捕获不同edge type之间的影响因素。GATNE-I同时考虑网络结构和节点属性，学习一个归纳变换函数，而不是直接学习每个节点的基嵌入和重嵌入。对于Alibaba数据集，我们使用与metapath2vec相同的元路径方案。对于一些没有节点属性的数据集，我们也为它们生成节点特征。由于Alibaba数据集超过4000万节点、5亿边的数据集规模以及其他竞争对手的可扩展性，我们只将我们的GATNE模型与DeepWalk、MVE、MNE进行比较。具体的实现可以在附录中找到。

#### 5.3	性能分析

链接预测在学术界和工业界都是一个常见的任务。对于学术界来说，它被广泛用于评价通过不同方法获得的网络嵌入质量。在行业中，链接预测是一项非常艰巨的任务，因为在现实场景中，我们通常会面对带有部分链接的图表，特别是对于依赖于用户和商品之间的链接进行推荐的电子商务公司。我们从原始图中隐藏一组边/非边，并对剩余的图进行训练。在[无监督归纳学习2,18]之后，我们创建了一个验证/测试集，其中分别包含5%/10%随机选择的正边，每种边类型随机选择的负边数量相同。验证集用于超参数调优和早期停止。测试集用于评估性能，并且只在调优的超参数下运行一次。我们在实验中使用了一些常用的评价标准，即ROC曲线下面积(ROC- auc)[12]和PR曲线(PR- auc)[5]。我们也使用F1分数作为评估的其他指标。为了避免[37]的阈值效应，我们假设测试集中隐藏边的数量是已知的[27,29,37]。所有这些指标在选定的边缘类型中都是均匀平均的。

**Quantitative Results 定量结果** 三个公共数据集和阿里巴巴s的实验结果如表4所示。在不同的数据集中，GATNE的性能优于各种基线。在Amazon数据集上，由于节点属性的限制，ggate - t的性能优于GATNE-I。由于阿里巴巴数据集节点属性丰富，使得ggate - i的性能最好。ANRL对弱节点属性非常敏感，在Amazon数据集上得到最差的结果。用户和物品的不同节点属性也限制了anrl在阿里巴巴数据集上的性能。在YouTube和Twitter数据集上，GATNE-I的表现与GATNE-T相似，因为这两个数据集的节点属性都是DeepWalk的节点嵌入，这些节点嵌入是由网络结构产生的。表5列出了阿里巴巴数据集的实验结果。GATNE算法在阿里巴巴数据集上具有很好的扩展性，在PR-AUC、ROC-AUC和F1-score上的性能提升分别为2.18%、5.78%和5.99%，与之前最先进算法的最佳结果相比。在大规模数据集中，GATNE-I比GATNE-T模型表现更好，这表明归纳方法在大规模的归因多路异质网络中工作得更好，这在现实世界中通常是这样的情况。

<img src="https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-11%2021.25.46.png" alt="截屏2021-02-11 21.25.46" style="zoom:90%;" />

**收敛性分析** 我们在阿里巴巴数据集上分析了我们提出的模型的收敛性。如图3(a)所示的结果表明，在非常大规模的真实世界数据集上，GATNE-I比GATNE-T收敛更快，并获得更好的性能。

**可扩充性分析** 为了进行优化，我们研究了部署在多个worker上的GATNE的可伸缩性。图3(b)显示了阿里巴巴数据集上worker数量的加速。从图中可以看出，GATNE在分布式平台上具有很强的可扩展性，因为当我们将worker数量相加时，培训时间会显著减少，最终归纳模型在2小时内就可以收敛到150个分布式worker。我们还发现，GATNE-I的培训速度几乎随worker数量的增加而线性增加，但少于150。而GATNE-T的收敛速度较慢，当worker数超过100时，其培训速度将达到极限。除了最先进的性能，GATNE还具有足够的可扩展性，可以在实践中采用。

![截屏2021-02-16 19.52.56](https://tva1.sinaimg.cn/large/e6c9d24egy1h1f25ebgm7j20ig084js2.jpg)

**参数灵敏度** 我们研究了GATNE中不同超参数的灵敏度，包括总体嵌入维数d和edge embedding 维数s。图4显示了在默认设置(d = 200, s = 10)的base embedding 维数d或edge embedding 维数s改变GATNE的性能。我们可以得出这样的结论:在大范围的base/edge embedding尺寸范围内，GATNE的性能是相对稳定的，当base/edge embedding太小或太大时，性能会下降。

#### 5.4	离线A/B测试

我们在阿里巴巴的分布式云平台上部署了我们的归纳模型GATNE-I作为推荐系统。该训练数据集拥有约1亿用户和1000万个项目，每天有100亿用户之间的交互。我们利用该模型生成用户和物品的嵌入向量。对于每个用户，我们使用K个最近邻(KNN)和欧氏距离来计算用户最可能点击的前n个条目。实验的目标是最大化top-N命中率。在A/B测试框架下，我们对GATNE-I、MNE、DeepWalk进行了离线测试。结果表明，与MNE和DeepWalk相比，GATNE-I算法的命中率分别提高了3.26%和24.26%。

### 6	结论

在我们的论文中，我们形式化的归属多重异质网络嵌入问题，并提出了GATNE解决它的传导和感应设置。我们将GATNE-I节点的整体嵌入分为三个部分:基础嵌入、边缘嵌入和属性嵌入。不同类型边缘之间共享基嵌入和属性嵌入，边缘嵌入是利用自注意机制对邻域信息进行聚合来计算的。在跨多个具有挑战性的数据集的链路预测任务上，我们提出的方法比以前的最先进的方法取得了显著更好的性能。该方法已在阿里巴巴的推荐系统中成功部署并进行了评价，具有良好的可扩展性和有效性。

### 致谢

### 参考文献

### A	附录

在附录中，我们首先给出我们建议模型的实施说明。然后给出了数据集的详细描述和所有方法的参数配置。最后，讨论了公平比较的问题和今后的工作。

#### A.1	实现注意事项

**运行环境** 本文的实验可分为两部分。其中一个是在4个数据集上进行的，使用一台Linux服务器，4个Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz, 512G RAM和8个NVIDIA Tesla 100-sxm2-16gb。本部分中提出的模型的代码是在Python 3.6中使用TensorFlow$^5$ 1.12实现的。另一部分是利用阿里巴巴的分布式云平台对整个阿里巴巴数据集进行研究，该平台包含数千名员工。每两个工作人员共用一个16GB内存的NVIDIA Tesla P100 GPU。我们提出的模型是在本部分使用Python 2.7中的TensorFlow 1.4实现的。

**实施细节** 我们在单个Linux服务器上使用的代码可以分为三个部分:随机游走、模型训练和评估。随机游走部分是参照deepwalk$^6$和metapath2vec$^7$的相应部分实现的。模型的训练部分是参照TensorFlow教程$^8$中的word2vec部分实现的。评价部分使用了scikit-learn$^9$的一些度量函数，包括roc_auc_score、f1_score、precision_recall_curve、auc。采用随机梯度下降法和Adam更新规则[17]对模型参数进行更新和优化。基于阿里巴巴分布式云平台的编码规则，实现了我们提出的模型的分布式版本，以实现分布式效率的最大化。高级API，如tf.estimator和tf。数据，用于阿里巴巴分布式云平台中计算资源利用率较高的数据。

**函数选择** 在我们的实验中，公式(1)中的许多不同的聚合器函数，如均值聚合器(Cf. Equation(2))或池化聚合器(Cf. Equation(3))，都达到了类似的性能。最后在我们模型的定量实验中使用了均值聚集器。我们使用线性变换函数作为归纳模型GATNE-I(13)式中属性$h_z$、$g_{z,r}$的参数化函数。

**参数配置** 们的基本/整体嵌入尺寸d设置为200，边缘嵌入的尺寸s设置为10。每个节点的遍历次数设置为20，遍历长度设置为10。为了生成节点上下文，窗口大小被设置为5。将每个正训练样本的负样本数L设为5个。将最大纪重数设置为50，如果验证集上的ROC-AUC在1个训练纪重内没有改善，我们的模型将提前停止。对于每个边缘类型r，系数$α_r$和$β_r$均设为1。对于阿里巴巴数据集，节点类型包括U和I，分别代表用户和商品。我们的方法的元路径方案设置为U-I-U和I-U-I。我们在TensorFlow中使用了adam优化器的默认设置;学习速率设置为lr = 0.001。对于5.4节中的离线A/B测试，我们使用N = 50。

**代码和数据集发布细节** 我们提出的模型的代码在单一的Linux服务器上(基于Tensorflow 1.12)，以及我们的三个公共数据集和阿里巴巴数据集的分区都可以得到。

#### A.2	比较方法

给出所有比较方法的详细运行配置。所有方法的嵌入尺寸都设置为200。对于基于随机游走的方法，将每个节点的游走次数设置为20，游走长度设置为10。为了生成节点上下文，窗口大小被设置为5。每个训练对的负样本个数设为5。训练skip-gram模型的迭代次数设置为100。下面将解释比较方法的代码源和其他特定超参数设置。

##### A.2.1	网络嵌入方法

* **DeepWalk深度游走** 对于公共数据集和Alibaba-S的数据集，我们使用了来自通讯作者[GitHub](https://github.com/phanein/deepwalk)$^6$的代码。对于Alibaba-S数据集，我们在阿里巴巴分布式云平台上重新实现了深度游走。
* **LINE** LINE的代码来自通讯作者[GitHub](https://github.com/tangjianpku/LINE)$^{10}$。我们使用直线(1st +2ed)作为整体嵌入。对于一阶和二阶嵌入，嵌入尺寸都设置为100。样本数量设置为10亿。
* **node2vec** node2vec的代码来自对应作者[GitHub](https://github.com/aditya-grover/node2vec)$^{11}$。Node2vec添加了两个参数来控制随机遍历过程。在我们的实验中，参数p设置为2，参数q设置为0.5。

##### A.2.2	异质网络嵌入方法

* **metapath2vec** 通信作者提供的代码仅针对特定的数据集，不能直接推广到其他数据集。基于原来的[c++代码](https://ericdongyx.github.io/metapath2vec/m2v.html)$^{12}$，我们在Python中为具有任意节点类型的网络重新实现了metapath2vec。当三个公共数据集的节点类型为1时，metapath2vec在这三个数据集中退化为DeepWalk。对于阿里巴巴数据集，节点类型包括U和I，分别代表用户和项。元路径方案分为U-I-U和I-U-I。

##### A.2.3 	多重异质网络嵌入方法

* **PMNE** 

  PMNE提出了三种不同的方法将node2vec应用于多路复用网络。根据MNE[43]的表示，将其网络聚合算法、结果聚合算法、层共分析算法分别表示为PMNE(n)、PMNE(r)、PMNE(c)。我们使用的代码来自MNE的[GitHub](https://github.com/HKUST-KnowComp/MNE)$^{13}$。遍历PMNE(c)各层的概率设为0.5。

* **MVE** 

  MVE使用协作上下文嵌入，并对特定于视图的嵌入应用注意机制。通信作者通过电子邮件收到了MVE的代码。每个视图的嵌入尺寸设置为200。每个epoch的训练样本个数设置为1亿，epoch的个数设置为10。对于Alibaba-S数据集，我们在阿里巴巴分布式云平台上重新实现了该方法。

* **MNE** 

  MNE对每个节点使用每种边缘类型的一个公共嵌入和几个附加嵌入，通过统一的网络嵌入模型共同学习。MNE的附加嵌入尺寸设置为10。我们使用通信作者在[GitHub](https://github.com/HKUST-KnowComp/MNE)$^{13}$中发布的代码。对于阿里巴巴数据集，我们在阿里巴巴分布式云平台上重新实现。

##### A.2.4	属性网络嵌入方法

* **ANRL** 我们使用的代码来自Alibaba的[GitHub](https://github.com/cszhangzhen/ANRL)$^{14}$。由于YouTube和Twitter数据集没有节点属性，我们为它们生成节点属性。具体来说，我们使用DeepWalk的节点嵌入(200维)作为ANRL在这些数据集上的输入节点特征。对于Alibaba和亚马逊的数据集，我们使用原始特征作为属性。

#### A.3	数据集

我们的实验在5个数据集上进行评估，包括4个数据集和Alibaba数据集。由于单个Linux服务器上内存和计算资源的限制，这四个数据集是从原始数据集采样的子图，用于训练和评估。表6显示了原始公共数据集的统计数据。

![22](https://gitee.com/jittors/picture/raw/master/uPic/%E6%88%AA%E5%B1%8F2021-02-12%2019.04.09.png)

* **Amazon** 是来自Amazon的产品评论和重数据的数据集。在我们的实验中，我们只使用产品重数据，包括产品属性和产品之间的共同查看、共同购买链接。Amazon的节点类型集合为O = {product}，Amazon的边缘类型集合为R = {also_bouдht, also_viewed}，表示同一用户分别共同购买或共同观看两种产品。Amazon的产品被分成许多类别。所有类别的产品数量如此之多，以至于我们使用电子类产品作为实验。对于许多算法来说，电子产品的数量仍然很大;因此，我们从整个图中提取一个连通子图。
* **YouTube** 是一个多维度的双向网络数据集，包含15088个YouTube用户之间的5种交互类型。边缘的类型包括联系人、共享好友、共享订阅、共享订阅者和用户之间共享喜爱的视频。它是|O| = 1和|R| = 5的多路网络。
* **Twitter** 是一个关于2012年7月1日至7日在Twitter上发布的关于希格斯玻色子发现的推文数据集。它由超过45万Twitter用户之间的4种定向关系组成。这些关系包括Twitter用户之间的转发、回复、提及和友谊/追随者关系。它是|O| = 1和|R| = 4的多路网络。
* **Alibaba** 有4种交互类型，包括点击、添加到偏好、添加到购物车，以及用户和商品两种节点之间的转换。Alibaba的节点类型集合为O = {user, item}，边缘类型集合的大小为|R| = 4。Alibaba的全图是如此之大，我们无法用一台机器来评估不同方法在它上的表现。我们从整个图中提取一个子图，用不同的方法进行比较，记为Alibaba-S。同时，我们也提供了对阿里巴巴分布式云平台的全图评价，全图记为Aliabab。



#### A.4	讨论

在网络嵌入的研究中，很多人利用链路预测或节点分类任务来评估网络嵌入的表示。然而，尽管有许多常用的公共数据集，如Twitter或YouTube数据集，但没有一个提供“标准”数据集;对不同任务进行培训、验证和测试的分离。这导致在同一数据集上不同的评价分离结果不同，因此不能直接使用以前论文的结果，研究者不得不自己重新实现和运行所有的基线，减少了他们对改进模型的注意力。

在这里，我们呼吁研究人员提供标准化的数据集，其中包含一个标准分离的训练、验证和测试集以及完整的数据集。因此研究者可以在一个标准的环境下评估他们的方法，并且可以直接比较论文的结果。这也有助于增加研究的可重复性。

**未来工作** 除了网络的异质性，网络的动态性也是网络表示学习的关键。捕获网络动态信息有三种方法。首先，我们可以在节点属性中添加动态信息。例如，我们可以使用像LSTM[14]这样的方法来捕获用户的动态活动。其次，动态信息，如每次交互的时间戳，可以看作是边缘的属性。第三，我们可以考虑代表网络动态演化的几个网络快照。我们将表示学习留给动态属性多重异质网络作为我们未来的工作。